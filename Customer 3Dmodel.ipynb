{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guangdi_mask_3D_directly.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "id": "i2P2z229MN-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF1qljLJdw6O"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import skimage, os\n",
        "from skimage.morphology import ball, disk, dilation, binary_erosion, remove_small_objects, erosion, closing, reconstruction, binary_closing\n",
        "from skimage.measure import label,regionprops, perimeter\n",
        "from skimage.morphology import binary_dilation, binary_opening\n",
        "from skimage.filters import roberts, sobel\n",
        "from skimage import measure, feature\n",
        "from skimage.segmentation import clear_border\n",
        "from skimage import data\n",
        "from scipy import ndimage as ndi\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from skimage.io import imread\n",
        "import re\n",
        "from random import shuffle\n",
        "import random\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import matplotlib.animation as anim\n",
        "\n",
        "import imageio\n",
        "from skimage.transform import resize\n",
        "\n",
        "import copy\n",
        "from scipy import ndimage as nd\n",
        "import nibabel as nib\n",
        "import itertools\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, concatenate, Conv3D, MaxPooling3D, Conv3DTranspose, AveragePooling3D, ZeroPadding3D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.metrics import AUC\n",
        "\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN9-H74Qd_LE"
      },
      "source": [
        "all_images = glob(os.path.join('/content/drive/MyDrive/Data/guangdi_1/Hospital A','*.nii.gz'))\n",
        "df = pd.read_excel('/content/drive/MyDrive/Data/guangdi_1/clinic.xlsx', sheet_name='Hospital A')\n",
        "\n",
        "all_images_b = glob(os.path.join('/content/drive/MyDrive/Data/guangdi_1/Hospital B','*.nii.gz'))\n",
        "df_val = pd.read_excel('/content/drive/MyDrive/Data/guangdi_1/clinic.xlsx', sheet_name='Hospital B')\n",
        "\n",
        "len(all_images), df.shape[0], len(all_images_b), df_val.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J25pyZ8HeEF6"
      },
      "source": [
        "def match_img(row, t='train'):\n",
        "\n",
        "  img_path = np.nan\n",
        "  if t == 'train':\n",
        "    name = '_'.join(row['Name'].strip().upper().split(' '))\n",
        "    cur_all_images = all_images\n",
        "  else:\n",
        "    name = '_'.join(row['缩写'].strip().upper().split(' '))\n",
        "    cur_all_images = all_images_b\n",
        "\n",
        "  for img in cur_all_images:\n",
        "    org_img = img\n",
        "    img = img.split('/')[-1]\n",
        "    f_i = re.search(r\"\\d\", img).start()\n",
        "    cur_name = img[: f_i-1]\n",
        "    if name == cur_name:\n",
        "      img_path = org_img\n",
        "      break\n",
        "\n",
        "  return img_path\n",
        "\n",
        "df['img_path'] = df.apply(lambda x: match_img(x, 'train'), axis=1)\n",
        "df_val['img_path'] = df_val.apply(lambda x: match_img(x, 'test'), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09USf9WfeZnR"
      },
      "source": [
        "show_ids = np.random.randint(df.shape[0])\n",
        "print(df.iloc[show_ids]['Name'])\n",
        "print(df.iloc[show_ids]['IDx'])\n",
        "print(df.iloc[show_ids]['img_path'])\n",
        "\n",
        "print('------------------------------')\n",
        "show_ids = np.random.randint(df_val.shape[0])\n",
        "print(df_val.iloc[show_ids]['缩写'])\n",
        "print(df_val.iloc[show_ids]['img_path'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G00HiRdDedzP"
      },
      "source": [
        "df = df.dropna().reset_index(drop=True)\n",
        "df['label'] = df['预后'].apply(lambda x: int(1) if x == 2 else int(0))\n",
        "df = df[['Name', 'img_path', 'label']]\n",
        "\n",
        "df_val = df_val.fillna(method='ffill')\n",
        "df_val['label'] = df_val['预后'].apply(lambda x: int(1) if x == 2 else int(0))\n",
        "df_val = df_val[['缩写','img_path', 'label']]\n",
        "\n",
        "df.shape, df_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa-B6ac_e0xL"
      },
      "source": [
        "img_rows = 128\n",
        "img_cols = 128\n",
        "img_depth = 64\n",
        "\n",
        "\n",
        "def resize_volume(image_p):\n",
        "    img = copy.deepcopy(image_p)\n",
        "    \"\"\"Resize across z-axis\"\"\"\n",
        "    # Set the desired depth\n",
        "    desired_depth = img_depth\n",
        "    desired_width = img_rows\n",
        "    desired_height = img_cols\n",
        "    z_ids = np.linspace(0, img.shape[2]-1, desired_depth)\n",
        "    z_ids = list(map(lambda x: int(x), z_ids))\n",
        "    img3d = np.stack([cv2.resize(img[:, :, i], (desired_width, desired_height)) for i in z_ids]).T\n",
        "    if np.min(img3d) < np.max(img3d):\n",
        "      img3d = img3d - np.min(img3d)\n",
        "      img3d = img3d / np.max(img3d)\n",
        "    if img3d.shape[-1] < desired_depth:\n",
        "      n_zero = np.zeros((desired_height, desired_width, desired_depth - img3d.shape[-1]))\n",
        "      img3d = np.concatenate((img3d,  n_zero), axis = -1)\n",
        "    return img3d\n",
        "\n",
        "\n",
        "def load_dicom_images_3d(img_path):\n",
        "\n",
        "    img3d = nib.load(img_path).get_fdata()\n",
        "    img3d = resize_volume(img3d)\n",
        "\n",
        "    return np.expand_dims(img3d,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhxXA3oUe9Xc"
      },
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class Dataset(Sequence):\n",
        "    def __init__(self,df,is_train=True,batch_size=2,shuffle=True):\n",
        "        self.idx = df.index\n",
        "        self.paths = df[\"img_path\"].values\n",
        "        self.y = df['label'].values\n",
        "        self.is_train = is_train\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.idx)/self.batch_size)\n",
        "   \n",
        "    def __getitem__(self,ids):\n",
        "        id_path= self.paths[ids]\n",
        "        batch_paths = self.paths[ids * self.batch_size:(ids + 1) * self.batch_size]\n",
        "        \n",
        "        if self.y is not None:\n",
        "            batch_y = self.y[ids * self.batch_size: (ids + 1) * self.batch_size]\n",
        "        \n",
        "        if self.is_train:\n",
        "            list_x =  [load_dicom_images_3d(x) for x in batch_paths]\n",
        "            batch_X = np.stack(list_x, axis=0)\n",
        "            return batch_X, batch_y\n",
        "        else:\n",
        "            list_x = load_dicom_images_3d(id_path)\n",
        "            batch_X = np.stack(list_x, axis=0)\n",
        "            return batch_X\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle and self.is_train:\n",
        "            ids_y = list(zip(self.idx, self.y))\n",
        "            shuffle(ids_y)\n",
        "            self.idx, self.y = list(zip(*ids_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def get_all_data_for_train():\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  labels = pd.get_dummies(df['label']).values\n",
        "  for ids, row in tqdm(df.iterrows()):\n",
        "    paths = row['img_path']\n",
        "    x = load_dicom_images_3d(paths)\n",
        "    if x.shape != (img_rows, img_cols, img_depth, 1):\n",
        "      continue\n",
        "    x_train.append(x)\n",
        "    y_train.append(labels[ids])\n",
        "  \n",
        "  return np.array(x_train), np.array(y_train)\n",
        "\n",
        "x_train, y_train = get_all_data_for_train()\n",
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "zk-hf56YMcm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se6j7nbxjVYv"
      },
      "source": [
        "def check_image_mask(row):\n",
        "  img_path = row['img_path']\n",
        "\n",
        "  img = load_dicom_images_3d(img_path)\n",
        "\n",
        "  return 1 if img.shape == (img_rows, img_cols, img_depth, 1) else 0\n",
        "\n",
        "df['check'] = df.apply(check_image_mask, axis=1)\n",
        "df_val['check'] = df_val.apply(check_image_mask, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KViquw6DmM7w"
      },
      "source": [
        "df = df[df['check'] == 1].reset_index(drop=True)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzTFPYrBfmDC"
      },
      "source": [
        "train_dataset = Dataset(df)\n",
        "val_dataset = Dataset(df_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVoag78IjCGP"
      },
      "source": [
        "show_ids = np.random.randint(100)\n",
        "images, labels = train_dataset[show_ids]\n",
        "print(\"Dimension of the img is:\", images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "def make_model(hp):\n",
        "    inputs = keras.Input((img_rows, img_cols, img_depth, 1))\n",
        "\n",
        "    x = keras.layers.Conv3D(filters=hp.Int('units_Conv_1_' + str(0),\n",
        "                                            min_value=16,\n",
        "                                            max_value=64,\n",
        "                                            step=4),\n",
        "                            kernel_size=3,\n",
        "                            activation=\"relu\", padding='same',\n",
        "                            name=\"Conv_1\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.layers.MaxPool3D(pool_size=2, padding='same')(x)\n",
        "\n",
        "    conv_1 = x\n",
        "\n",
        "    x = keras.layers.Conv3D(filters=hp.Int('units_conv2_' + str(1),\n",
        "                                            min_value=16,\n",
        "                                            max_value=128,\n",
        "                                            step=4),\n",
        "                            kernel_size=3,\n",
        "                            activation=\"relu\", padding='same',\n",
        "                            name=\"Conv_2\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.layers.MaxPool3D(pool_size=2, padding='same')(x)\n",
        "\n",
        "\n",
        "    con3_f = hp.Int('units_conv3_' + str(2), min_value=32, max_value=256, step=4)\n",
        "\n",
        "    x = keras.layers.Conv3D(filters=con3_f,\n",
        "                            kernel_size=3,\n",
        "                            activation=\"relu\", padding='same',\n",
        "                            name=\"Conv_3\")(x)\n",
        "    x1 = keras.layers.Conv3D(filters=con3_f, kernel_size=1, strides=2, activation=\"relu\", padding='same', name=\"Conv_input\")(conv_1)\n",
        "    x = keras.layers.concatenate([x, x1])\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = keras.layers.MaxPool3D(pool_size=1, strides=4, padding='same')(x)\n",
        "    \n",
        "    x = layers.Dropout(\n",
        "        hp.Float('dense_dropout', min_value=0., max_value=0.7)\n",
        "    )(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = layers.Dense(\n",
        "        units=hp.Int('num_dense_units', min_value=8, max_value=64, step=8),\n",
        "        activation='relu'\n",
        "    )(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    cat_acc = tf.keras.metrics.BinaryAccuracy(name='acc')\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[cat_acc]\n",
        "    )\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "-8iZ-EvdNjob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner\n",
        "\n",
        "tuner = kt.tuners.BayesianOptimization(\n",
        "    make_model,\n",
        "    objective=keras_tuner.Objective(\"val_acc\", direction=\"max\"),\n",
        "    max_trials=10, \n",
        "    overwrite=True)\n",
        "\n",
        "callbacks=[keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=5, baseline=0.9)]\n",
        "\n",
        "tuner.search(x_train, y_train, validation_split=0.1 ,callbacks=callbacks, verbose=1, epochs=50, batch_size=2)"
      ],
      "metadata": {
        "id": "rs-pOHxTNk58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hp = tuner.get_best_hyperparameters()[0]\n",
        "best_model = make_model(best_hp)\n",
        "keras.utils.plot_model(best_model, show_shapes=True)"
      ],
      "metadata": {
        "id": "u6RKAo-rbVVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acIKITs-kx-b"
      },
      "source": [
        "def get_model(width=128, height=128, depth=64):\n",
        "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Define the model.\n",
        "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model.\n",
        "model = get_model(width=img_rows, height=img_cols, depth=img_depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2j1hSTBk3Zm"
      },
      "source": [
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "# tf.keras.metrics.AUC(name='auc')\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    metrics=[tf.keras.metrics.AUC(name='auc')],\n",
        ")\n",
        "\n",
        "# Define callbacks.\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"3d_image_classification.h5\", save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=15)\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch\n",
        "epochs = 100\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQB9m-l21SKb"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "test_dataset = Dataset(df_val, is_train=False, batch_size=1)\n",
        "\n",
        "preds = []\n",
        "for i in tqdm(range(len(test_dataset))):\n",
        "  preds.append(model.predict(np.expand_dims(test_dataset[i], axis=0))[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT17T20QyYhy"
      },
      "source": [
        "avg_pred = np.mean(preds)\n",
        "avg_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbvb3smI1Z9T"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_test = df_val['label']\n",
        "preds_int = []\n",
        "\n",
        "for v in preds:\n",
        "  if v > avg_pred:\n",
        "    preds_int.append(1)\n",
        "  else:\n",
        "    preds_int.append(0)\n",
        "\n",
        "print(classification_report(y_test, preds_int))\n",
        "print(f'AUC is {roc_auc_score(y_test, preds)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
