{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuuyuZ925qaU"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import skimage, os\n",
    "from skimage.morphology import ball, disk, dilation, binary_erosion, remove_small_objects, erosion, closing, reconstruction, binary_closing\n",
    "from skimage.measure import label,regionprops, perimeter\n",
    "from skimage.morphology import binary_dilation, binary_opening\n",
    "from skimage.filters import roberts, sobel\n",
    "from skimage import measure, feature\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage import data\n",
    "from scipy import ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from skimage.io import imread\n",
    "import re\n",
    "from random import shuffle\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as anim\n",
    "\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "\n",
    "import copy\n",
    "from scipy import ndimage as nd\n",
    "import nibabel as nib\n",
    "import itertools\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.layers import Input, concatenate, Conv3D, MaxPooling3D, Conv3DTranspose, AveragePooling3D, ZeroPadding3D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.metrics import AUC\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiRlFPco52hg",
    "outputId": "b81ca888-aeff-4461-bb78-4e06ee3318a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 499, 500, 500)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = glob(os.path.join('/content/drive/MyDrive/Data/guangdi_1/Hospital A','*.nii.gz'))\n",
    "all_masks = glob(os.path.join('/content/drive/MyDrive/Data/guangdi_1/mask_A','*.nii.gz'))\n",
    "df = pd.read_excel('/content/drive/MyDrive/Data/guangdi_1/clinic.xlsx', sheet_name='Hospital A')\n",
    "roi = pd.read_excel('/content/drive/MyDrive/Data/guangdi_1/volInfo_mask_A_20211112131836.xlsx', sheet_name='Sheet1')\n",
    "len(all_images), len(all_masks), df.shape[0], roi.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xc5aAFa6Aou"
   },
   "outputs": [],
   "source": [
    "def match_img_mask(row):\n",
    "  img_path = np.nan\n",
    "  mask_path = np.nan\n",
    "  name = '_'.join(row['Name'].strip().upper().split(' '))\n",
    "\n",
    "  for img in all_images:\n",
    "    org_img = img\n",
    "    img = img.split('/')[-1]\n",
    "    f_i = re.search(r\"\\d\", img).start()\n",
    "    cur_name = img[: f_i-1]\n",
    "    if name == cur_name:\n",
    "      img_path = org_img\n",
    "      break\n",
    "  \n",
    "  for mask in all_masks:\n",
    "    org_mask = mask\n",
    "    mask = mask.split('/')[-1]\n",
    "    f_i = re.search(r\"\\d\", mask).start()\n",
    "    cur_name = mask[: f_i-1]\n",
    "    if name == cur_name:\n",
    "      mask_path = org_mask\n",
    "      break\n",
    "\n",
    "  return pd.Series([img_path, mask_path])\n",
    "\n",
    "df[['img_path', 'mask_path']] = df.apply(match_img_mask, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiclqAm6zMO8"
   },
   "outputs": [],
   "source": [
    "def extract_name(x):\n",
    "  f_i = re.search(r\"\\d\", x).start()\n",
    "  x = x[: f_i-1]\n",
    "  x = x.lower().split('_')\n",
    "  x = list(map(lambda x: x.strip(), x))\n",
    "  x = list(filter(lambda x: len(x) > 0, x))\n",
    "  return ' '.join(x)\n",
    "\n",
    "def format_name(x):\n",
    "  x = x.lower().split(' ')\n",
    "  x = list(map(lambda x: x.strip(), x))\n",
    "  x = list(filter(lambda x: len(x) > 0, x))\n",
    "  return ' '.join(x)\n",
    "\n",
    "roi['name'] = roi['fileName'].apply(lambda x: extract_name(x))\n",
    "roi = roi[['name', 'vol_roi1', 'vol_roi2', 'vol_roi3']]\n",
    "df['Name'] = df['Name'].apply(lambda x: format_name(x))\n",
    "df = df.sort_values(by=['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "q_tfrLXd40tE",
    "outputId": "d8719086-84c5-4d76-f725-0d849fb97e07"
   },
   "outputs": [],
   "source": [
    "roi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA4CkqEo2Azy"
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, roi, left_on='Name', right_on='name', how='left')\n",
    "df[['vol_roi1', 'vol_roi2', 'vol_roi3']] = df[['vol_roi1', 'vol_roi2', 'vol_roi3']].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EamCcZcW6HPA",
    "outputId": "7a324225-fb72-402a-eefa-000af0b7b8ca"
   },
   "outputs": [],
   "source": [
    "show_ids = np.random.randint(df.shape[0])\n",
    "print(df.iloc[show_ids]['缩写'])\n",
    "print(df.iloc[show_ids]['IDx'])\n",
    "print(df.iloc[show_ids]['img_path'])\n",
    "print(df.iloc[show_ids]['mask_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RWU-TAJ6Nh4"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['name'], inplace=True)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.columns = ['r_no', 'name', 'f1', 'f2', 'label', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'img_path', 'mask_path', 'vol_roi1', 'vol_roi2', 'vol_roi3']\n",
    "df['label'] = df['label'].apply(lambda x: int(1) if x == 2 else int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "Larzgb9m76de",
    "outputId": "c47a8c4c-718d-4a0b-df13-d737c8481670"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tab_cols = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'vol_roi1', 'vol_roi2', 'vol_roi3']\n",
    "scaler = MinMaxScaler()\n",
    "df[tab_cols] = scaler.fit_transform(df[tab_cols])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5imb2RSg9Skt"
   },
   "outputs": [],
   "source": [
    "img_rows = 400\n",
    "img_cols = 400\n",
    "img_depth = 16\n",
    "\n",
    "\n",
    "def resize_volume(image_p):\n",
    "    img = copy.deepcopy(image_p)\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = img_depth\n",
    "    desired_width = img_rows\n",
    "    desired_height = img_cols\n",
    "    z_ids = np.linspace(0, img.shape[2]-1, desired_depth)\n",
    "    z_ids = list(map(lambda x: int(x), z_ids))\n",
    "    img3d = np.stack([cv2.resize(img[:, :, i], (desired_width, desired_height)) for i in z_ids]).T\n",
    "    if np.min(img3d) < np.max(img3d):\n",
    "      img3d = img3d - np.min(img3d)\n",
    "      img3d = img3d / np.max(img3d)\n",
    "    if img3d.shape[-1] < desired_depth:\n",
    "      n_zero = np.zeros((desired_height, desired_width, desired_depth - img3d.shape[-1]))\n",
    "      img3d = np.concatenate((img3d,  n_zero), axis = -1)\n",
    "    return img3d\n",
    "\n",
    "def load_dicom_images_3d(img_path):\n",
    "\n",
    "    img3d = nib.load(img_path).get_fdata()\n",
    "\n",
    "    img3d = resize_volume(img3d)\n",
    "\n",
    "    return np.expand_dims(img3d,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BhpXuSbOxEj"
   },
   "outputs": [],
   "source": [
    "def check_image_mask(row):\n",
    "  img_path = row['img_path']\n",
    "  mask_path = row['img_path']\n",
    "\n",
    "  img = load_dicom_images_3d(img_path)\n",
    "  mask = load_dicom_images_3d(mask_path)\n",
    "\n",
    "  return 1 if img.shape == (400, 400, 16, 1) and mask.shape == (400, 400, 16, 1) else 0\n",
    "\n",
    "df['check'] = df.apply(check_image_mask, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yojbHl4OTPuE"
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NX3Nde6aS8A5"
   },
   "outputs": [],
   "source": [
    "df = df[df['check'] == 1].reset_index(drop=True)\n",
    "df.to_csv('clean_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6apRTJUMPu3D",
    "outputId": "2c34bb3d-124c-4c9d-e0f7-fe2bd927e49c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((450, 23), (40, 23))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df.iloc[:450]\n",
    "val_df = df.iloc[450:]\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqCJd0Vi_wau"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class Dataset(Sequence):\n",
    "    def __init__(self,df,is_train=True,batch_size=2,shuffle=True,tab_cols=tab_cols):\n",
    "        self.idx = df.index\n",
    "        self.paths = df[\"img_path\"].values\n",
    "        self.masks = df['mask_path'].values\n",
    "        self.y = df['label'].values\n",
    "        self.tab = df[tab_cols].values\n",
    "        self.is_train = is_train\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.idx)/self.batch_size)\n",
    "   \n",
    "    def __getitem__(self,ids):\n",
    "        id_path= self.paths[ids]\n",
    "        id_mask_path = self.masks[ids]\n",
    "\n",
    "        batch_paths = self.paths[ids * self.batch_size:(ids + 1) * self.batch_size]\n",
    "        batch_mask_paths = self.masks[ids * self.batch_size:(ids + 1) * self.batch_size]\n",
    "        \n",
    "        if self.y is not None:\n",
    "            batch_y = self.y[ids * self.batch_size: (ids + 1) * self.batch_size]\n",
    "        \n",
    "        if self.is_train:\n",
    "            list_x =  [load_dicom_images_3d(x) for x in batch_paths]\n",
    "            list_mask = [load_dicom_images_3d(x) for x in batch_mask_paths]\n",
    "            batch_tab = self.tab[ids * self.batch_size: (ids + 1) * self.batch_size]\n",
    "            batch_X = np.stack(list_x, axis=0)\n",
    "            batch_mask = np.stack(list_mask, axis=0)\n",
    "            return [batch_X, batch_tab], [batch_mask, batch_y]\n",
    "        else:\n",
    "            list_x = load_dicom_images_3d(id_path)\n",
    "            batch_X = np.stack(list_x)\n",
    "            batch_tab = self.tab[ids * self.batch_size: (ids + 1) * self.batch_size]\n",
    "            return batch_X, batch_tab\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle and self.is_train:\n",
    "            ids_y = list(zip(self.idx, self.y))\n",
    "            shuffle(ids_y)\n",
    "            self.idx, self.y = list(zip(*ids_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33v79TvgBALX"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_df)\n",
    "valid_dataset = Dataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9m1r04MVgNW",
    "outputId": "cd2753a1-f0c7-49a8-b700-1e6c764857da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the img is: (2, 400, 400, 16, 1)\n",
      "Dimension of the mask is: (2, 400, 400, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "show_ids = np.random.randint(100)\n",
    "[images, tabs], [masks, labels] = train_dataset[show_ids]\n",
    "print(\"Dimension of the img is:\", images.shape)\n",
    "print(\"Dimension of the mask is:\", masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWZS2x5rC3n2"
   },
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    inputs_img = Input((img_depth, img_rows, img_cols, 1))\n",
    "  \n",
    "    conv1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(inputs_img)\n",
    "    conv1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv3DTranspose(256, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv5), conv4], axis=4)\n",
    "    conv6 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv6), conv3], axis=4)\n",
    "    conv7 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv7), conv2], axis=4)\n",
    "    conv8 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv3DTranspose(32, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv8), conv1], axis=4)\n",
    "    conv9 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "\n",
    "    output_img = Conv3D(1, (1, 1, 1), activation='softmax', name='mask')(conv9)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(conv5)\n",
    "    img_x = layers.Dense(units=128, activation=\"relu\")(x)\n",
    "\n",
    "    inputs_tab = Input(shape=(17,))\n",
    "    input_x = layers.Dense(units=256, activation=\"relu\")(inputs_tab)\n",
    "\n",
    "    x = concatenate([img_x, input_x])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    residual_x = x\n",
    "    for _ in range(4):\n",
    "      x = layers.Dense(units=384, activation=\"relu\")(x)\n",
    "      x = layers.Dropout(0.25)(x)\n",
    "      x = layers.add([x, residual_x])\n",
    "      residual_x = x\n",
    "\n",
    "\n",
    "    output_label = layers.Dense(units=1, activation=\"sigmoid\", name='label')(x)\n",
    "\n",
    "\n",
    "    model = keras.Model(inputs=[inputs_img, inputs_tab], outputs=[output_img, output_label])\n",
    "\n",
    "    keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    model.compile(loss={'label': 'binary_crossentropy', \n",
    "                        'mask': 'mean_squared_error'},\n",
    "                  loss_weights = {\"label\": 9, \"mask\": 1},\n",
    "                  optimizer='adam',\n",
    "                  metrics={'label': keras.metrics.BinaryAccuracy(name='acc')})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwiYTqtMFe0t",
    "outputId": "a222e1ff-4103-488f-f7df-ade42715172f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 14.5366 - mask_loss: 0.9966 - label_loss: 1.5044 - label_acc: 0.5489WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 536s 2s/step - loss: 14.5366 - mask_loss: 0.9966 - label_loss: 1.5044 - label_acc: 0.5489 - val_loss: 26.2126 - val_mask_loss: 0.9957 - val_label_loss: 2.8019 - val_label_acc: 0.3250\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 10.3270 - mask_loss: 0.9966 - label_loss: 1.0367 - label_acc: 0.5378WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 10.3270 - mask_loss: 0.9966 - label_loss: 1.0367 - label_acc: 0.5378 - val_loss: 10.4803 - val_mask_loss: 0.9957 - val_label_loss: 1.0538 - val_label_acc: 0.5500\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 8.6128 - mask_loss: 0.9966 - label_loss: 0.8462 - label_acc: 0.5022WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 8.6128 - mask_loss: 0.9966 - label_loss: 0.8462 - label_acc: 0.5022 - val_loss: 7.7903 - val_mask_loss: 0.9957 - val_label_loss: 0.7550 - val_label_acc: 0.5250\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 8.6747 - mask_loss: 0.9966 - label_loss: 0.8531 - label_acc: 0.5044WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 8.6747 - mask_loss: 0.9966 - label_loss: 0.8531 - label_acc: 0.5044 - val_loss: 48.0507 - val_mask_loss: 0.9957 - val_label_loss: 5.2283 - val_label_acc: 0.4250\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 8.2292 - mask_loss: 0.9966 - label_loss: 0.8036 - label_acc: 0.5178WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 8.2292 - mask_loss: 0.9966 - label_loss: 0.8036 - label_acc: 0.5178 - val_loss: 10.4511 - val_mask_loss: 0.9957 - val_label_loss: 1.0506 - val_label_acc: 0.5000\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 8.1257 - mask_loss: 0.9966 - label_loss: 0.7921 - label_acc: 0.4800WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 8.1257 - mask_loss: 0.9966 - label_loss: 0.7921 - label_acc: 0.4800 - val_loss: 8.4374 - val_mask_loss: 0.9957 - val_label_loss: 0.8269 - val_label_acc: 0.4500\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.7192 - mask_loss: 0.9966 - label_loss: 0.7469 - label_acc: 0.5556WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 7.7192 - mask_loss: 0.9966 - label_loss: 0.7469 - label_acc: 0.5556 - val_loss: 9.1984 - val_mask_loss: 0.9957 - val_label_loss: 0.9114 - val_label_acc: 0.5500\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.9347 - mask_loss: 0.9966 - label_loss: 0.7709 - label_acc: 0.5333WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 7.9347 - mask_loss: 0.9966 - label_loss: 0.7709 - label_acc: 0.5333 - val_loss: 9.1877 - val_mask_loss: 0.9957 - val_label_loss: 0.9102 - val_label_acc: 0.4500\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.6963 - mask_loss: 0.9966 - label_loss: 0.7444 - label_acc: 0.5111WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 7.6963 - mask_loss: 0.9966 - label_loss: 0.7444 - label_acc: 0.5111 - val_loss: 8.5525 - val_mask_loss: 0.9957 - val_label_loss: 0.8396 - val_label_acc: 0.4500\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.6186 - mask_loss: 0.9966 - label_loss: 0.7358 - label_acc: 0.4867WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 7.6186 - mask_loss: 0.9966 - label_loss: 0.7358 - label_acc: 0.4867 - val_loss: 63786.6328 - val_mask_loss: 0.9957 - val_label_loss: 7087.2930 - val_label_acc: 0.3750\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.6874 - mask_loss: 0.9966 - label_loss: 0.7434 - label_acc: 0.4844WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 7.6874 - mask_loss: 0.9966 - label_loss: 0.7434 - label_acc: 0.4844 - val_loss: 21085.1211 - val_mask_loss: 0.9957 - val_label_loss: 2342.6809 - val_label_acc: 0.3750\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.5205 - mask_loss: 0.9966 - label_loss: 0.7249 - label_acc: 0.5067WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 536s 2s/step - loss: 7.5205 - mask_loss: 0.9966 - label_loss: 0.7249 - label_acc: 0.5067 - val_loss: 6244.4741 - val_mask_loss: 0.9957 - val_label_loss: 693.7198 - val_label_acc: 0.3750\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.5015 - mask_loss: 0.9966 - label_loss: 0.7228 - label_acc: 0.4911WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 533s 2s/step - loss: 7.5015 - mask_loss: 0.9966 - label_loss: 0.7228 - label_acc: 0.4911 - val_loss: 40.7196 - val_mask_loss: 0.9957 - val_label_loss: 4.4138 - val_label_acc: 0.3750\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.4896 - mask_loss: 0.9966 - label_loss: 0.7214 - label_acc: 0.4889WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 533s 2s/step - loss: 7.4896 - mask_loss: 0.9966 - label_loss: 0.7214 - label_acc: 0.4889 - val_loss: 351.2266 - val_mask_loss: 0.9957 - val_label_loss: 38.9145 - val_label_acc: 0.3750\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 7.4831 - mask_loss: 0.9966 - label_loss: 0.7207 - label_acc: 0.5067WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,mask_loss,label_loss,label_acc,val_loss,val_mask_loss,val_label_loss,val_label_acc\n",
      "225/225 [==============================] - 532s 2s/step - loss: 7.4831 - mask_loss: 0.9966 - label_loss: 0.7207 - label_acc: 0.5067 - val_loss: 92.8942 - val_mask_loss: 0.9957 - val_label_loss: 10.2109 - val_label_acc: 0.3750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eea269610>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_net()\n",
    "\n",
    "model_save = ModelCheckpoint('model.h5', \n",
    "                             save_best_only = True, \n",
    "                             monitor = 'val_acc', \n",
    "                             mode = 'max', verbose = 1)\n",
    "early_stop = EarlyStopping(monitor = 'val_acc', \n",
    "                           patience = 10, mode = 'max', verbose = 1,\n",
    "                           restore_best_weights = True)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=15,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks = [model_save, early_stop],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "guangdi_mask_mutli-task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
